# ü§ñ LLM\_Connect Plugin for Unreal Engine

**LLM\_Connect** is a powerful Unreal Engine plugin that enables seamless integration with Large Language Models (LLMs) via Blueprints. It supports **OpenAI (GPT)**, **Google Gemini**, **Anthropic Claude**, **DeepSeek**, and **Ollama (local or remote)** ‚Äî all through a unified Blueprint node. You can send prompts and receive streamed or full responses in real time. No C++ coding is required.

---

## ‚ú® Features

* üîπ **Unified Blueprint Node**: Communicate with multiple LLMs using a single Blueprint node.
* üîπ **Multi-LLM Support**: Easily connect to popular models:

  * OpenAI **GPT**
  * Google **Gemini**
  * **Claude** by Anthropic
  * **DeepSeek**
  * **Ollama** (local models)
* üîπ **Dropdown Model Selection**: Choose from supported model names.
* üîπ **Flexible Input**: Configure API Key, Prompt, Model Name, Server IP/Port, and more.
* üîπ **Streaming Parsing**: Supports streamed output (Ollama).
* üîπ **Text-only Smart Extraction**: Auto-extract meaningful responses from LLMs.

---

## üè¶ 1. Installation (via Fab Marketplace)

1. Go to the **Fab Marketplace**.
2. Search for **LLM Connect**.
3. Purchase or add it to your Library.
4. Open the **Epic Games Launcher**, go to the **Library**, and install it to your Engine or Project.

---

## üìö 2. Adding to Your Unreal Project

1. Open your Unreal project.
2. Go to `Edit ‚Üí Plugins`.
3. Find **LLM Connect** under Installed and enable it.
4. Restart the project when prompted.

---

## ü§ñ 3. Using the Plugin (Blueprint Only)

1. Open any Blueprint (e.g., Level Blueprint, Actor Blueprint).
2. Right-click in the Event Graph and search for `LLM Connect`.
3. Select the node: **LLM Connect ‚Äì Unified LLM Request**.

---

## ‚öõÔ∏è 4. Node Pin Reference

| Pin                      | Type    | Description                                                                                                                                                                                                                                                                      |
| ------------------------ | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Model Type**           | Enum    | Select the LLM service:<br>‚Ä¢ GPT (OpenAI)<br>‚Ä¢ Gemini (Google)<br>‚Ä¢ Claude (Anthropic)<br>‚Ä¢ DeepSeek<br>‚Ä¢ Ollama (local)                                                                                                                                                         |
| **Model**                | String  | The model to use.<br>*Examples*:<br>‚Ä¢ `gpt-4`, `gpt-3.5-turbo` (GPT)<br>‚Ä¢ `gemini-pro`, `gemini-1.5-flash`, `gemini-1.5-pro` (Gemini)<br>‚Ä¢ `claude-3-opus`, `claude-3-sonnet` (Claude)<br>‚Ä¢ `deepseek-chat`, `deepseek-coder` (DeepSeek)<br>‚Ä¢ `llama3`, `mistral`, etc. (Ollama) |
| **API Key**              | String  | API key required for GPT, Gemini, Claude, and DeepSeek.                                                                                                                                                                                                                          |
| **Prompt**               | String  | The question to send to the LLM.                                                                                                                                                                                                                                                 |
| **Server IP/Port**       | String  | For Ollama only. Example: `http://localhost:11434`.                                                                                                                                                                                                                              |
| **Max Tokens**           | Integer | Used with Claude to limit response length.                                                                                                                                                                                                                                       |
| **Temperature**          | Float   | (0.0 to 1.0) Controls randomness.                                                                                                                                                                                                                                                |
| **System Prompt**        | String  | Optional role description (e.g., "You are a helpful assistant.").                                                                                                                                                                                                                |
| **On Success**           | Exec    | Triggered when a response is received.                                                                                                                                                                                                                                           |
| **Response Text**        | String  | The LLM's returned message.                                                                                                                                                                                                                                                      |
| **Standard Exec Output** | Exec    | Normal flow output.                                                                                                                                                                                                                                                              |

---

## üìà 5. API Behavior Table

| Model    | Requires API Key | Needs Server | Notes                                      |
| -------- | ---------------- | ------------ | ------------------------------------------ |
| GPT      | ‚úÖ Yes            | ‚ùå No         | e.g., `gpt-3.5-turbo`, `gpt-4`             |
| Gemini   | ‚úÖ Yes            | ‚ùå No         | e.g., `gemini-1.5-flash`, `gemini-1.5-pro` |
| Claude   | ‚úÖ Yes            | ‚ùå No         | Needs `Max Tokens`                         |
| DeepSeek | ‚úÖ Yes            | ‚ùå No         | e.g., `deepseek-chat`                      |
| Ollama   | ‚ùå No             | ‚úÖ Yes        | Server must be running locally.            |

---

## üìÇ 6. Blueprint Example

> **Blueprint Flow**
>
> `Begin Play` ‚ûî `LLM Connect Node`
>
> Configure:
>
> * **Model Type**: GPT
> * **Model**: `gpt-3.5-turbo`
> * **Prompt**: "What is the capital of France?"
> * **API Key**: \[Your API Key]
> * **On Success**: Connect to `Print String` using `Response Text`

---

## üí° 7. Additional Notes

* Ollama requires local installation and model setup.
* Ollama does **not** use API Keys.
* Other LLMs require **valid API Keys** for access.
* All config data is runtime-only ‚Äì nothing is saved.
* Full source code is included.

---

## ‚öñÔ∏è 8. Support & Documentation

* GitHub: [https://github.com/yigit-altun-rootrootq/LLM-Connect](https://github.com/yigit-altun-rootrootq/LLM-Connect)
* Contact: [https://www.youtube.com/@rootrootQ](https://www.youtube.com/@rootrootQ)

---

¬© LLM Connect Plugin - 2025. All Rights Reserved.
