# ü§ñ LLM_Connect Plugin for Unreal Engine

**LLM_Connect** is a robust Unreal Engine plugin designed to integrate large language models (LLMs) with Unreal Engine's Blueprint system. By enabling seamless communication with models like OpenAI's GPT, Google's Gemini, Claude by Anthropic, DeepSeek, and Ollama, LLM_Connect allows you to incorporate advanced AI capabilities into your projects with ease. No C++ coding is required ‚Äì everything is handled within Blueprints.

---

## ‚ú® Features

- üîπ **Unified Blueprint Node**: A single Blueprint node that simplifies sending prompts and receiving responses from various LLMs.
- üîπ **Multi-LLM Support**: Easily connect to popular models:
  - Google **Gemini**
  - OpenAI **GPT**
  - **Claude** by Anthropic
  - **DeepSeek**
  - **Ollama** models
- üîπ **Dropdown Model Selection**: Choose from a list of available models without needing to manually configure them.
- üîπ **API Key, Prompt, Model Name, and Server Input**: Full flexibility in configuration.
- üîπ **Streaming Parsing Support**: Specifically for Ollama models, allows you to stream and parse responses in real-time.
- üîπ **Text-only Smart Extraction**: Efficiently extracts meaningful content from text responses generated by the LLM.

---

## üéÆ Use Cases

- **AI-Powered NPC Conversations**: Bring life to your NPCs by integrating dynamic conversations powered by LLMs.
- **Dynamic Quest Generation**: Create unique quests with LLM-based generation techniques.
- **Developer Tools**: Automate development tasks such as code generation or in-game script creation.
- **Text-to-Text Generation**: Use the plugin for summarization, content generation, dialogue scripting, and more.

---

## üì¶ Installation

Follow these steps to install the **LLM_Connect** plugin into your Unreal Engine project:

1. **Download the plugin**: Obtain the plugin files.
2. **Place the plugin** in your UE project‚Äôs `Plugins/` directory.
3. **Regenerate Project Files**: After adding the plugin, regenerate the project files through Unreal Engine's IDE or command line.
4. **Build the Plugin**: Build the plugin and ensure there are no compilation errors.
5. **Enable the Plugin**:
   - Open your project in Unreal Engine.
   - Navigate to `Edit > Plugins`.
   - Find **LLM_Connect** in the list and enable it.
6. **Restart Unreal Engine**: Restart UE to ensure the plugin is properly loaded and ready for use.

---

## üß† How It Works

### üî∑ Blueprint Node: `SendLLMRequest`

The core functionality of the **LLM_Connect** plugin is encapsulated in the Blueprint node `SendLLMRequest`. This node connects your Unreal Engine project to LLMs for generating AI-driven text responses. Below is a description of each input and output pin of the node:

| Pin              | Type   | Description                                                                 |
|------------------|--------|-----------------------------------------------------------------------------|
| **LLM Type**     | Enum   | Select the LLM (e.g., GPT, Gemini, Claude, DeepSeek, Ollama).               |
| **API Key**      | String | The API key for the selected LLM (not required for Ollama).                 |
| **Prompt**       | String | The text prompt/question you wish to send to the LLM.                        |
| **Model**        | String | The specific model to use (e.g., `gpt-4`, `gemini`, `claude-opus`).          |
| **Server IP:Port** | String | (Optional for Ollama) Server address and port to connect to Ollama models. |
| **Max Tokens**   | Int    | Optional token limit for models like Claude (controls the length of the response). |
| **OnSuccess**    | Output | The event triggered when the LLM successfully returns a response.           |
| **Result**       | String | The parsed AI response (e.g., text output from the LLM).                    |

---

## üß© Blueprint Example

Here is an example of how to set up and use the `SendLLMRequest` node in Blueprints:

> üìå Below is an example of the `SendLLMRequest` node setup:

![LLM Node Blueprint Example](/node_example.png)

In the Blueprint, connect the node inputs as follows:

1. **LLM Type**: Choose the LLM you want to use (e.g., `GPT`, `Claude`).
2. **API Key**: If necessary, input your LLM API key (for Ollama, leave this blank).
3. **Prompt**: Define the prompt that you want to send to the model.
4. **Model**: Specify the model (e.g., `gpt-4`).
5. **Max Tokens**: (Optional) Set the maximum number of tokens (useful for Claude).
6. **OnSuccess**: Link to an event or function to handle the successful AI response.
7. **Result**: Capture the AI-generated response and use it within your game or application.

---

## üõ† Advanced Configuration

### Streaming Support (For Ollama Models)

LLM_Connect supports **streaming parsing** for Ollama models. This feature allows you to receive and process AI responses incrementally. It is particularly useful for generating long responses where you want to display the text in real-time or process the data as it arrives.

### Model Selection

**LLM_Connect** simplifies the model selection process by providing a dropdown menu in the Blueprint node. Choose your model directly from the list of supported models without worrying about detailed configuration. Supported models include:
- **GPT** (OpenAI)
- **Gemini** (Google)
- **Claude** (Anthropic)
- **DeepSeek**
- **Ollama**

### Handling Large Responses

For models like GPT and Claude, responses may be long. You can configure the **Max Tokens** parameter to limit the length of the responses, preventing excessive data from being returned. This is especially useful in real-time applications where performance is critical.

---

## üíº Purchase

The **LLM_Connect** plugin is available for purchase on **Fab**.  
‚û°Ô∏è [**Buy Now on Fab**](https://www.fab.com/sellers/rootrootQ)

---

## üôå Created by

This plugin is developed by **rootrootQ**.  
For updates, tutorials, and support, visit my [YouTube channel @rootrootQ](https://www.youtube.com/rootrootQ).

---

## ‚ùì FAQ

- **How do I get an API key for GPT, Gemini, or Claude?**
  - You need to sign up for the respective service (e.g., OpenAI, Google Cloud, Anthropic) and obtain an API key from their developer console.
  
- **Can I use this plugin without an API key?**
  - Yes, Ollama models do not require an API key. For other models, an API key is required.
  
- **How can I stream responses from Ollama models?**
  - Ensure that the **Streaming Parsing Support** option is enabled in the node, and the plugin will handle streaming for you.

